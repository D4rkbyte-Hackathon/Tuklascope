import re
import os
import json
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image

load_dotenv()

try:
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
except Exception as e:
    print(f"Error configuring Gemini API: {e}")
    exit()

# Helper function to read prompt files
def _read_prompt(file_path: str) -> str:
    # Reads the content of a prompt file.
    # Returns: The content of the file, or None if the file is not found.
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"Error: Prompt file not found at {file_path}")
        return None
    except Exception as e:
        print(f"Error reading prompt file {file_path}: {e}")
        return None

# --- AI FUNCTIONS ---

def get_identified_object(image_path: str) -> dict | None:
    # Identifies the main object in an image using the 'ai_object_identification.prompt'.
    # accepts the image_path or the file path to the image to be analyzed.
    # Returns: A dictionary containing 'object_label' and 'category_hint', or None if identification fails.
    
    prompt_path = os.path.join('prompts', 'ai_object_identification.prompt')
    prompt_template = _read_prompt(prompt_path)
    if not prompt_template:
        return None

    try:
        img = Image.open(image_path)

        model = genai.GenerativeModel('gemini-1.5-flash') # ------------------------------------para dali makit an
        response = model.generate_content([prompt_template, img])

        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        cleaned_text = match.group(0) if match else response.text

        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        print(f"Error: Failed to decode JSON from identification model. Raw response: {response.text}")
        return None
    except Exception as e:
        print(f"An error occurred in get_identified_object: {e}")
        return None

def generate_spark_content(object_info: dict, grade_level: str) -> dict | None:
    # Generates educational content for the 'Spark' feature (Quick Facts, STEM Concepts, Hands-On Project) using the 'ai_spark_content_generation.prompt'.
    # Accepts: object_info which is a dictionary containing 'object_label' and 'category_hint' & the grade_level - the user's registered grade level ("Elementary", "Junior High School", "Senior High School").
    # Returns: A dictionary containing 'quick_facts', 'stem_concepts', and 'hands_on_project', or None if content generation fails.

    prompt_path = os.path.join('prompts', 'ai_spark_content_generation.prompt')
    prompt_template = _read_prompt(prompt_path)
    if not prompt_template:
        return None

    formatted_prompt = prompt_template.format(
        grade_level=grade_level,
        user_location="Philippines", # sabutan pa kay basin mobati if sobraan ka specific
        object_label=object_info.get('object_label'),
        category_hint=object_info.get('category_hint')
    )

    try:
        model = genai.GenerativeModel('gemini-2.5-pro')  # ------------------------------------para dali makit an
        response = model.generate_content(formatted_prompt)

        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        cleaned_text = match.group(0) if match else response.text

        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        print(f"Error: Failed to decode JSON from content generation model. Raw response: {response.text}")
        return None
    except Exception as e:
        print(f"An error occurred in generate_spark_content: {e}")
        return None


def get_normalized_stem_skills(spark_content: dict) -> list[dict] | None:
    #  Analyzes the provided spark content to extract, normalize, and categorize STEM concepts into a structured format for the user's Skill Tree using the 'ai_skill_extraction.prompt'.
    # Accepts : spark_content - The dictionary containing 'quick_facts', 'stem_concepts', and 'hands_on_project' generated by generate_spark_content.
    # Returns: A list of dictionaries, where each dict has 'skill_name' (str) and 'category' (str), or None if extraction/normalization fails.
    
    prompt_path = os.path.join('prompts', 'ai_skill_extraction.prompt')
    prompt_template = _read_prompt(prompt_path)
    if not prompt_template:
        return None

    content_to_analyze = f"Quick Facts: {spark_content.get('quick_facts', '')}\n\n" \
                         f"STEM Concepts: {spark_content.get('stem_concepts', '')}\n\n" \
                         f"Hands-On Project: {spark_content.get('hands_on_project', '')}"

    formatted_prompt = prompt_template.format(content_to_analyze=content_to_analyze)

    try:
        model = genai.GenerativeModel('gemini-2.5-pro')  # ------------------------------------para dali makit an
        response = model.generate_content(formatted_prompt)

        # Robust JSON extraction
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        cleaned_text = match.group(0) if match else response.text

        output = json.loads(cleaned_text)
        
        if "normalized_skills" in output and isinstance(output["normalized_skills"], list):
            final_skills = []
            seen_skill_names = set()
            for skill_item in output["normalized_skills"]:
                if isinstance(skill_item, dict) and 'skill_name' in skill_item and 'category' in skill_item:
                    if skill_item['skill_name'] not in seen_skill_names:
                        final_skills.append(skill_item)
                        seen_skill_names.add(skill_item['skill_name'])
                else:
                    print(f"Warning: Malformed skill item in AI skill extraction output: {skill_item}. Skipping.")
            return final_skills
        else:
            print(f"Error: AI skill extraction output malformed or missing 'normalized_skills' key. Raw response: {response.text}")
            return None
        
    except json.JSONDecodeError:
        print(f"Error: Failed to decode JSON from AI skill extraction model. Raw response: {response.text}")
        return None
    except Exception as e:
        print(f"An error occurred in get_normalized_stem_skills: {e}")
        return None

def get_pathfinder_guidance(user_skills: dict, grade_level: str) -> dict | None:
    # Generates personalized academic/career guidance based on the user's STEM skill tree data and their current grade level, using the 'ai_pathfinder_guidance.prompt'.
    # Accepts: user_skills - A dictionary where keys are skill names (str) and values are mastery scores (int, typically 0-100). Example: {"Photosynthesis": 80, "Mechanics": 50, "Algebra": 70} & grade_level (str)
    # Returns: A structured dictionary containing recommendations (e.g., SHS strands, college programs, career paths), or None if guidance generation fails.
    
    prompt_path = os.path.join('prompts', 'ai_pathfinder_guidance.prompt')
    prompt_template = _read_prompt(prompt_path)
    if not prompt_template:
        return None

    user_skills_json = json.dumps(user_skills) if user_skills else "{}"

    current_date_time = datetime.now().strftime("%A, %B %d, %Y at %I:%M:%S %p %Z")

    formatted_prompt = prompt_template.format(
        grade_level=grade_level,
        user_skills_json=user_skills_json,
        current_date_time=current_date_time
    )

    try:
        model = genai.GenerativeModel('gemini-2.5-pro')  # ------------------------------------para dali makit an
        response = model.generate_content(formatted_prompt)

        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        cleaned_text = match.group(0) if match else response.text

        pathfinder_output = json.loads(cleaned_text)

        required_keys = ["title", "summary", "strongest_fields", "recommendations"]
        if not all(key in pathfinder_output for key in required_keys):
            print(f"Error: Pathfinder output missing required keys. Raw response: {response.text}")
            return None

        return pathfinder_output

    except json.JSONDecodeError:
        print(f"Error: Failed to decode JSON from Pathfinder model. Raw response: {response.text}")
        return None
    except Exception as e:
        print(f"An error occurred in get_pathfinder_guidance: {e}")
        return None

def get_ai_tutor_response(user_question: str, grade_level: str, chat_history: list[dict] = None, object_context: str = None) -> str | None:
    # Generates a conversational response from the AI tutor. It considers the current user question, their grade level, previous chat history, and the object they are currently viewing using 'ai_tutor_conversation.prompt'.
    # Accepts: 1. user_question (str)- the current question asked by the user, 
                # 2. grade_level (str), 
                # 3. chat_history (list[dict])(can be None) where each dict should have "role" ("user" or "model") and "parts" (string).
                # 4. object_context (str)(can also be null): The name of the object the user is currently viewing/learning about.
    # Returns: The AI tutor's response (str), or None if an error occurs.
    
    prompt_path = os.path.join('prompts', 'ai_tutor_conversation.prompt')
    prompt_template = _read_prompt(prompt_path)
    if not prompt_template:
        return None

    context_addon = f"The user is currently looking at content about a '{object_context}'. " if object_context else ""

    chat_history_json = json.dumps(chat_history if chat_history is not None else [], indent=2)

    current_date_time = datetime.now().strftime("%A, %B %d, %Y at %I:%M:%S %p %Z")

    formatted_prompt = prompt_template.format(
        grade_level=grade_level,
        user_question=user_question, # the *last* user question in the actual messages list
        context_addon=context_addon,
        chat_history_json=chat_history_json,
        current_date_time=current_date_time
    )

    try:
        model = genai.GenerativeModel('gemini-2.5-pro') # ------------------------------------para dali makit an
        
        messages = [
            {"role": "user", "parts": formatted_prompt}
        ]
        
        if chat_history:
            for message in chat_history:
                if message.get("role") in ["user", "model"] and "parts" in message:
                    messages.append({"role": message["role"], "parts": message["parts"]})
                else:
                    print(f"Warning: Malformed chat history message: {message}. Skipping.")

        # add the current user's question as the last turn
        messages.append({"role": "user", "parts": user_question})

        response = model.generate_content(messages)

        return response.text.strip()
    except Exception as e:
        print(f"An error occurred in get_ai_tutor_response: {e}")
        return None